{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import all the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   X  Y month  day  FFMC   DMC     DC  ISI  temp  RH  wind  rain  area\n",
      "0  7  5   mar  fri  86.2  26.2   94.3  5.1   8.2  51   6.7   0.0   0.0\n",
      "1  7  4   oct  tue  90.6  35.4  669.1  6.7  18.0  33   0.9   0.0   0.0\n",
      "2  7  4   oct  sat  90.6  43.7  686.9  6.7  14.6  33   1.3   0.0   0.0\n",
      "3  8  6   mar  fri  91.7  33.3   77.5  9.0   8.3  97   4.0   0.2   0.0\n",
      "4  8  6   mar  sun  89.3  51.3  102.2  9.6  11.4  99   1.8   0.0   0.0\n",
      "after area log transformation: \n",
      "0      0.009950\n",
      "1      0.009950\n",
      "2      0.009950\n",
      "3      0.009950\n",
      "4      0.009950\n",
      "         ...   \n",
      "195    2.304583\n",
      "196    2.391511\n",
      "197    2.403335\n",
      "198    2.419479\n",
      "199    2.426571\n",
      "Name: area, Length: 200, dtype: float64\n",
      "          X         Y      FFMC       DMC        DC       ISI      temp  \\\n",
      "0  1.008313  0.569860 -0.805959 -1.323326 -1.830477 -0.860946 -1.842640   \n",
      "1  1.008313 -0.244001 -0.008102 -1.179541  0.488891 -0.509688 -0.153278   \n",
      "2  1.008313 -0.244001 -0.008102 -1.049822  0.560715 -0.509688 -0.739383   \n",
      "3  1.440925  1.383722  0.191362 -1.212361 -1.898266 -0.004756 -1.825402   \n",
      "4  1.440925  1.383722 -0.243833 -0.931043 -1.798600  0.126966 -1.291012   \n",
      "\n",
      "         RH  month_apr  month_aug  ...  month_nov  month_oct  month_sep  \\\n",
      "0  0.411724        0.0        0.0  ...        0.0        0.0        0.0   \n",
      "1 -0.692456        0.0        0.0  ...        0.0        1.0        0.0   \n",
      "2 -0.692456        0.0        0.0  ...        0.0        1.0        0.0   \n",
      "3  3.233519        0.0        0.0  ...        0.0        0.0        0.0   \n",
      "4  3.356206        0.0        0.0  ...        0.0        0.0        0.0   \n",
      "\n",
      "   day_fri  day_mon  day_sat  day_sun  day_thu  day_tue  day_wed  \n",
      "0      1.0      0.0      0.0      0.0      0.0      0.0      0.0  \n",
      "1      0.0      0.0      0.0      0.0      0.0      1.0      0.0  \n",
      "2      0.0      0.0      1.0      0.0      0.0      0.0      0.0  \n",
      "3      1.0      0.0      0.0      0.0      0.0      0.0      0.0  \n",
      "4      0.0      0.0      0.0      1.0      0.0      0.0      0.0  \n",
      "\n",
      "[5 rows x 27 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from ucimlrepo import fetch_ucirepo \n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "import mlflow\n",
    "import mlflow.tensorflow\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import MeanSquaredError\n",
    "import mlflow.pyfunc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to create a Neural Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(input_dim):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(64, activation='relu', input_dim=input_dim))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dense(1))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the dataset from its repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch the dataset\n",
    "forest_fires = fetch_ucirepo(id=162)\n",
    "forestFiresDataframe = pd.DataFrame(forest_fires.data['features'])\n",
    "forestFiresDataframe['area'] = forest_fires.data['targets']\n",
    "print(forestFiresDataframe.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the categorical and numerical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categoricalFeatures = ['month', 'day']\n",
    "numericalFeatures = ['X', 'Y', 'FFMC', 'DMC', 'DC', 'ISI', 'temp', 'RH']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handle missing values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Although I noticed none in this dataset but it is a good practice to check for missing values :)\n",
    "imputer = SimpleImputer(strategy='most_frequent')\n",
    "forestFiresDataframe[categoricalFeatures] = imputer.fit_transform(forestFiresDataframe[categoricalFeatures])\n",
    "forestFiresDataframe[numericalFeatures] = imputer.fit_transform(forestFiresDataframe[numericalFeatures])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log transformation for target variable i.e. area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Area is skewed towards 0, so we apply a log transformation to it to make it more normally distributed\n",
    "forestFiresDataframe['area'] = forestFiresDataframe['area'].replace(0,1.01).fillna(1.01).apply(lambda x: np.log(x))\n",
    "print(\"After area log transformation: \")\n",
    "print(forestFiresDataframe['area'].head(200))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply one-hot encoding to categorical features, and scale numerical features\n",
    "# We'll end up with a total of 27 columns after one-hot encoding\n",
    "encoder = OneHotEncoder(handle_unknown='ignore')\n",
    "encodedCategoricalFeatures = encoder.fit_transform(forestFiresDataframe[categoricalFeatures]).toarray()\n",
    "\n",
    "# Scale numerical features\n",
    "scaler = StandardScaler()\n",
    "scaledNumericalFeatures = scaler.fit_transform(forestFiresDataframe[numericalFeatures])\n",
    "\n",
    "# Combine the processed features\n",
    "processedData = np.hstack((scaledNumericalFeatures, encodedCategoricalFeatures))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert to Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Convert to DataFrame for better readability\n",
    "processedDataframe = pd.DataFrame(processedData, columns=numericalFeatures + encoder.get_feature_names_out(categoricalFeatures).tolist())\n",
    "print(processedDataframe.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Divide features and target variable into separate variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X = processedDataframe\n",
    "y = forestFiresDataframe['area']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data into taining and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Split into training and test sets\n",
    "featuresTrain, featuresTest, targetsTrain, targetsTest = train_test_split(X, y, test_size=0.2, random_state=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an MLFlow experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='file:///c:/Users/sajid/Desktop/Flow/mlruns/895373400277021307', creation_time=1734545721621, experiment_id='895373400277021307', last_update_time=1734545721621, lifecycle_stage='active', name='neural_network_regression', tags={}>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We set this experiment before running the model, so mlflow can log the model to this experiment\n",
    "mlflow.set_experiment(\"neural_network_regression\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sajid\\.conda\\envs\\mlflowenv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 24ms/step - loss: 3.0879 - val_loss: 2.0703\n",
      "Epoch 2/50\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 2.5141 - val_loss: 2.0452\n",
      "Epoch 3/50\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1.8785 - val_loss: 2.1032\n",
      "Epoch 4/50\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 1.9966 - val_loss: 2.1197\n",
      "Epoch 5/50\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 2.0353 - val_loss: 2.1476\n",
      "Epoch 6/50\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 1.9232 - val_loss: 2.1300\n",
      "Epoch 7/50\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 1.8916 - val_loss: 2.1411\n",
      "Epoch 8/50\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1.7858 - val_loss: 2.1619\n",
      "Epoch 9/50\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 1.9046 - val_loss: 2.1535\n",
      "Epoch 10/50\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1.6773 - val_loss: 2.1670\n",
      "Epoch 11/50\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 1.8247 - val_loss: 2.1723\n",
      "Epoch 12/50\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 1.7186 - val_loss: 2.1809\n",
      "Epoch 13/50\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 1.7955 - val_loss: 2.1794\n",
      "Epoch 14/50\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 1.8063 - val_loss: 2.1963\n",
      "Epoch 15/50\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 1.5263 - val_loss: 2.1801\n",
      "Epoch 16/50\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 1.7149 - val_loss: 2.2091\n",
      "Epoch 17/50\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 1.6037 - val_loss: 2.2110\n",
      "Epoch 18/50\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 1.6791 - val_loss: 2.2277\n",
      "Epoch 19/50\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 1.6036 - val_loss: 2.2215\n",
      "Epoch 20/50\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 1.4324 - val_loss: 2.2347\n",
      "Epoch 21/50\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 1.4325 - val_loss: 2.2537\n",
      "Epoch 22/50\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 1.4746 - val_loss: 2.2549\n",
      "Epoch 23/50\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 1.4456 - val_loss: 2.2863\n",
      "Epoch 24/50\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 1.4920 - val_loss: 2.2700\n",
      "Epoch 25/50\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 1.5646 - val_loss: 2.2925\n",
      "Epoch 26/50\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 1.4628 - val_loss: 2.3222\n",
      "Epoch 27/50\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 1.3065 - val_loss: 2.3036\n",
      "Epoch 28/50\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 1.4085 - val_loss: 2.3383\n",
      "Epoch 29/50\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 1.3979 - val_loss: 2.3212\n",
      "Epoch 30/50\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 1.5490 - val_loss: 2.3682\n",
      "Epoch 31/50\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 1.3174 - val_loss: 2.3596\n",
      "Epoch 32/50\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 1.3420 - val_loss: 2.3818\n",
      "Epoch 33/50\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 1.3143 - val_loss: 2.3969\n",
      "Epoch 34/50\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 1.2490 - val_loss: 2.4294\n",
      "Epoch 35/50\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 1.3162 - val_loss: 2.4223\n",
      "Epoch 36/50\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 1.1707 - val_loss: 2.4224\n",
      "Epoch 37/50\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 1.1258 - val_loss: 2.4620\n",
      "Epoch 38/50\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1.2103 - val_loss: 2.4603\n",
      "Epoch 39/50\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1.1363 - val_loss: 2.4695\n",
      "Epoch 40/50\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1.1530 - val_loss: 2.5183\n",
      "Epoch 41/50\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 1.1443 - val_loss: 2.4861\n",
      "Epoch 42/50\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 1.0881 - val_loss: 2.5222\n",
      "Epoch 43/50\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 1.2327 - val_loss: 2.5531\n",
      "Epoch 44/50\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 1.0899 - val_loss: 2.5342\n",
      "Epoch 45/50\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 1.0067 - val_loss: 2.5576\n",
      "Epoch 46/50\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 1.1520 - val_loss: 2.6226\n",
      "Epoch 47/50\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1.1115 - val_loss: 2.5707\n",
      "Epoch 48/50\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.8540 - val_loss: 2.6131\n",
      "Epoch 49/50\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.9772 - val_loss: 2.6711\n",
      "Epoch 50/50\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 1.0219 - val_loss: 2.6462\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/12/19 17:23:14 WARNING mlflow.tensorflow: You are saving a TensorFlow Core model or Keras model without a signature. Inference with mlflow.pyfunc.spark_udf() will not work unless the model's pyfunc representation accepts pandas DataFrames as inference inputs.\n",
      "2024/12/19 17:23:26 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
     ]
    }
   ],
   "source": [
    "input_dim = featuresTrain.shape[1]\n",
    "epochs = 50\n",
    "batch_size = 32\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile and train neural network model while logging everything with MLFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# End any active MLflow run, faced some issues with this in the past so better end the previous run\n",
    "mlflow.end_run()\n",
    "\n",
    "# Start an MLflow run\n",
    "with mlflow.start_run():\n",
    "    # Log parameters\n",
    "    mlflow.log_param(\"input_dim\", input_dim)\n",
    "    mlflow.log_param(\"epochs\", epochs)\n",
    "    mlflow.log_param(\"batch_size\", batch_size)\n",
    "    mlflow.log_param(\"learning_rate\", learning_rate)\n",
    "\n",
    "    # Create and compile the model\n",
    "    model = create_model(input_dim)\n",
    "    model.compile(optimizer=Adam(learning_rate=learning_rate), loss=MeanSquaredError())\n",
    "\n",
    "    # Train the model\n",
    "    history = model.fit(featuresTrain, targetsTrain, epochs=epochs, batch_size=batch_size, validation_data=(featuresTest, targetsTest))\n",
    "\n",
    "    # Log the model\n",
    "    mlflow.tensorflow.log_model(model, \"model\")\n",
    "\n",
    "    # Log metrics\n",
    "    for epoch in range(epochs):\n",
    "        mlflow.log_metric(\"loss\", history.history['loss'][epoch], step=epoch)\n",
    "        mlflow.log_metric(\"val_loss\", history.history['val_loss'][epoch], step=epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make a test prediction on the logged model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 97ms/step\n",
      "          0\n",
      "0  0.458066\n"
     ]
    }
   ],
   "source": [
    "model_uri = 'runs:/157958cbd0e64c819a98fc9693844cd8/model'\n",
    "\n",
    "# Load the model\n",
    "model = mlflow.pyfunc.load_model(model_uri)\n",
    "\n",
    "# Define an input example\n",
    "INPUT_EXAMPLE = featuresTest.iloc[0].to_dict()\n",
    "\n",
    "# Convert the input example to a DataFrame\n",
    "input_df = pd.DataFrame([INPUT_EXAMPLE])\n",
    "\n",
    "# Validate the serving payload works on the model\n",
    "prediction = model.predict(input_df)\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Just a test prediction on the logged model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_EXAMPLE = featuresTest.iloc[0].to_dict()\n",
    "\n",
    "# Convert the input example to a DataFrame\n",
    "input_df = pd.DataFrame([INPUT_EXAMPLE])\n",
    "\n",
    "# Validate the serving payload works on the model\n",
    "prediction = model.predict(input_df)\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make predictions from the logged model, on the whole test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>393</th>\n",
       "      <td>0.458066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>0.841729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>493</th>\n",
       "      <td>0.139625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>289</th>\n",
       "      <td>0.659365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>0.824134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>241</th>\n",
       "      <td>-0.178453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.549066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.350644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>1.394855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>372</th>\n",
       "      <td>0.470088</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>104 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0\n",
       "393  0.458066\n",
       "196  0.841729\n",
       "493  0.139625\n",
       "289  0.659365\n",
       "166  0.824134\n",
       "..        ...\n",
       "241 -0.178453\n",
       "42   0.549066\n",
       "5    1.350644\n",
       "124  1.394855\n",
       "372  0.470088\n",
       "\n",
       "[104 rows x 1 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logged_model = 'runs:/157958cbd0e64c819a98fc9693844cd8/model'\n",
    "\n",
    "# Load model as a PyFuncModel.\n",
    "loaded_model = mlflow.pyfunc.load_model(logged_model)\n",
    "\n",
    "# Predict on a Pandas DataFrame.\n",
    "loaded_model.predict(featuresTest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Just another experiment with mlflow to log predictions file as an artifact on the current run of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n"
     ]
    }
   ],
   "source": [
    "logged_model = 'runs:/157958cbd0e64c819a98fc9693844cd8/model'\n",
    "\n",
    "# Load model as a PyFuncModel.\n",
    "loaded_model = mlflow.pyfunc.load_model(logged_model)\n",
    "\n",
    "# Predict on a Pandas DataFrame.\n",
    "import pandas as pd\n",
    "predictions = loaded_model.predict(featuresTest)\n",
    "\n",
    "# Save predictions to a CSV file\n",
    "predictions_df = predictions.rename(columns={0: \"predictions\"})\n",
    "predictions_df.to_csv(\"predictions.csv\", index=False)\n",
    "\n",
    "# Log the predictions file\n",
    "mlflow.log_artifact(\"predictions.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### So far, we're done with the training and logging of our model with MLFlow. We've trained our Neural Network and Logged it using MLFlow. The model metrics etc and its artifacts could be observed on the MLFlow UI as in the attached screenshots."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next Step is the deployment of this logged model which I have done using Streamlit. In streamlit code we'll use the run id of the model from this current run to get predictions from this trained model using unseen data (That unseen data is being retrieved from the user using the streamlit interface). Streamlit then communicates with the logged model and get prediction of forest fire area from that model using the input features which were entered by the user on the streamlit UI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### After deployment on localhost, I have tried various methods for cloud deployment. The first one being Streamlit's own cloud. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method to deploy on streamlit cloud is:\n",
    "1. Create an account on streamlit.io\n",
    "2. Push your whole code repo alongwith mlflow runs directory, to GitHub\n",
    "3. Click on Create app button on the top right of the streamlit.io page\n",
    "4. Now select Deploy a public app from Github\n",
    "5. Now just follow the directions to connect your github repo to streamlit\n",
    "6. After some time of being in the oven the app will be deployed to streamlit cloud\n",
    "7. You'll get a URL for the app, in previous steps while connecting github repo you could even set a custom URL like I did\n",
    "8. Just copy and paste the URL in any browser and you can see your app running and you can interact with your model using the streamlit interface and get predictions on fresh unseen data.\n",
    "\n",
    "Here's the link to my streamlit app for this model: https://ds24-as2-c24muhna.streamlit.app/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method to deploy using Ngrok\n",
    "First of all I don't prefer this method since it's an old school thing for just demonstration purposes, has no real industry value. I did it back in my Bachelors project for demonstration of my CNN model by making it interact with a mobile app.\n",
    "Here are the steps for ngrok\n",
    "1. Install ngrok on your system\n",
    "2. Navigate to the ngrok directory and activate it\n",
    "3. Save or export your model as an h5 file\n",
    "4. Ngrok will expose a port on your system to be accessible through internet via an ip that ngrok provides you. This IP is kind of temporary since it'll be valid as long as your system or ngrok doesn't halt.\n",
    "5. You need to configure a Flask API as well with ngrok, Flask will get predictions from the model while ngrok will serve those results over the IP:port that you exposed\n",
    "6. You can catch the results from the requests and show them anywhere like in a web app or a mobile app "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy on cloud using Docker image, GitHub Actions CI/CD and Azure Cloud\n",
    "1. Create a ci/cd pipleline, it's a yml file in the .github/workflows directory. The script in this file connects the docker hub image and the azure cloud and github is the mediator between them.\n",
    "2. As soon as we push a new code update, github will use this pipeline to build and deploy the docker image to Azure Cloud\n",
    "3. Make sure to properly configure your Azure Cloud credentials like clientid, clientsecret, dockerhub login id and password etc on GitHub repository's settings under Secrets option\n",
    "4. Now if all your configurations were correct then any new push to your github repo will trigger this ci/cd pipeline and your model will be built into docker image and deployed on Azure cloud "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Please refer to the PDF file for screenshots of different steps in MLFlow UI and Deployment of Model for predictions on fresh unseen data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlflowenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
